{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86a8e91",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4bdeb1",
   "metadata": {},
   "source": [
    "In this final notebook, we'll integrate everything we've learned by building a complete data analysis workflow. We'll:\n",
    "\n",
    "1. Work with real-world data analysis libraries (NumPy, Pandas, Matplotlib)\n",
    "2. Create custom modules for our analysis\n",
    "3. Organize code into a logical package structure\n",
    "4. Apply best practices for imports and documentation\n",
    "5. Build a reproducible analysis pipeline\n",
    "\n",
    "**Project**: Analyzing sales data to identify trends, seasonality, and anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d7ca1",
   "metadata": {},
   "source": [
    "## Part 1: Using Data Analysis Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3943413",
   "metadata": {},
   "source": [
    "### Standard Import Conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68979719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports for data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b7ea7f",
   "metadata": {},
   "source": [
    "### Generate Sample Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sales data for demonstration\n",
    "def generate_sales_data(n_days=365):\n",
    "    \"\"\"\n",
    "    Generate synthetic sales data with trends and seasonality.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_days : int\n",
    "        Number of days to generate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Sales data with date, product, quantity, and revenue\n",
    "    \"\"\"\n",
    "    # Date range\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    dates = [start_date + timedelta(days=i) for i in range(n_days)]\n",
    "    \n",
    "    # Products\n",
    "    products = ['Product_A', 'Product_B', 'Product_C', 'Product_D']\n",
    "    base_prices = {'Product_A': 29.99, 'Product_B': 49.99, \n",
    "                   'Product_C': 19.99, 'Product_D': 39.99}\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        # Weekly seasonality (weekend boost)\n",
    "        day_of_week = date.weekday()\n",
    "        weekend_factor = 1.3 if day_of_week >= 5 else 1.0\n",
    "        \n",
    "        # Monthly trend (growing business)\n",
    "        month_factor = 1 + (date.month / 12) * 0.2\n",
    "        \n",
    "        for product in products:\n",
    "            # Base quantity with randomness\n",
    "            base_qty = random.randint(10, 30)\n",
    "            quantity = int(base_qty * weekend_factor * month_factor + np.random.normal(0, 3))\n",
    "            quantity = max(0, quantity)  # No negative quantities\n",
    "            \n",
    "            # Revenue\n",
    "            price = base_prices[product] * (1 + np.random.normal(0, 0.05))  # Price variation\n",
    "            revenue = quantity * price\n",
    "            \n",
    "            data.append({\n",
    "                'date': date,\n",
    "                'product': product,\n",
    "                'quantity': quantity,\n",
    "                'price': price,\n",
    "                'revenue': revenue\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Add some missing values (realistic scenario)\n",
    "    missing_indices = np.random.choice(df.index, size=int(len(df) * 0.02), replace=False)\n",
    "    df.loc[missing_indices, 'quantity'] = np.nan\n",
    "    \n",
    "    # Add some outliers (data quality issues)\n",
    "    outlier_indices = np.random.choice(df.index, size=5, replace=False)\n",
    "    df.loc[outlier_indices, 'quantity'] *= 3\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_sales_data(365)\n",
    "\n",
    "print(\"Sales Data Generated\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e02d76",
   "metadata": {},
   "source": [
    "## Part 2: Creating Custom Analysis Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073001e",
   "metadata": {},
   "source": [
    "### Module 1: Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be in a separate file: data_quality.py\n",
    "\n",
    "def check_missing_values(df):\n",
    "    \"\"\"\n",
    "    Analyze missing values in DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Summary of missing values by column\n",
    "    \"\"\"\n",
    "    missing_count = df.isnull().sum()\n",
    "    missing_percent = (missing_count / len(df)) * 100\n",
    "    \n",
    "    missing_df = pd.DataFrame({\n",
    "        'missing_count': missing_count,\n",
    "        'missing_percent': missing_percent\n",
    "    })\n",
    "    \n",
    "    return missing_df[missing_df['missing_count'] > 0].sort_values('missing_count', ascending=False)\n",
    "\n",
    "def detect_outliers_iqr(series):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : pd.Series\n",
    "        Numerical data series\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with outlier information\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    \n",
    "    return {\n",
    "        'count': len(outliers),\n",
    "        'percentage': (len(outliers) / len(series)) * 100,\n",
    "        'lower_bound': lower_bound,\n",
    "        'upper_bound': upper_bound,\n",
    "        'outlier_values': outliers.tolist()\n",
    "    }\n",
    "\n",
    "def data_quality_report(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality report.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to analyze\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Quality report with various metrics\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'missing_values': check_missing_values(df).to_dict(),\n",
    "        'duplicates': df.duplicated().sum(),\n",
    "        'memory_usage': df.memory_usage(deep=True).sum() / 1024**2  # MB\n",
    "    }\n",
    "    \n",
    "    # Check outliers in numerical columns\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    outliers = {}\n",
    "    for col in numerical_cols:\n",
    "        outliers[col] = detect_outliers_iqr(df[col].dropna())\n",
    "    report['outliers'] = outliers\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Test the functions\n",
    "print(\"Data Quality Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing = check_missing_values(df)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing)\n",
    "\n",
    "print(\"\\nOutliers in Quantity:\")\n",
    "outlier_info = detect_outliers_iqr(df['quantity'].dropna())\n",
    "print(f\"Count: {outlier_info['count']} ({outlier_info['percentage']:.2f}%)\")\n",
    "print(f\"Bounds: [{outlier_info['lower_bound']:.2f}, {outlier_info['upper_bound']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee42c7c",
   "metadata": {},
   "source": [
    "### Module 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56938aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be in: preprocessing.py\n",
    "\n",
    "def handle_missing_values(df, strategy='mean', columns=None):\n",
    "    \"\"\"\n",
    "    Handle missing values in DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with missing values\n",
    "    strategy : str\n",
    "        Strategy for filling: 'mean', 'median', 'forward_fill', 'drop'\n",
    "    columns : list, optional\n",
    "        Specific columns to process (None = all)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with handled missing values\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if columns is None:\n",
    "        columns = df_copy.columns\n",
    "    \n",
    "    for col in columns:\n",
    "        if df_copy[col].isnull().any():\n",
    "            if strategy == 'mean':\n",
    "                df_copy[col].fillna(df_copy[col].mean(), inplace=True)\n",
    "            elif strategy == 'median':\n",
    "                df_copy[col].fillna(df_copy[col].median(), inplace=True)\n",
    "            elif strategy == 'forward_fill':\n",
    "                df_copy[col].fillna(method='ffill', inplace=True)\n",
    "            elif strategy == 'drop':\n",
    "                df_copy.dropna(subset=[col], inplace=True)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def remove_outliers(df, column, method='iqr', threshold=1.5):\n",
    "    \"\"\"\n",
    "    Remove outliers from DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame to process\n",
    "    column : str\n",
    "        Column to check for outliers\n",
    "    method : str\n",
    "        Method: 'iqr' or 'zscore'\n",
    "    threshold : float\n",
    "        Threshold for outlier detection\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame without outliers\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    if method == 'iqr':\n",
    "        Q1 = df_copy[column].quantile(0.25)\n",
    "        Q3 = df_copy[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - threshold * IQR\n",
    "        upper_bound = Q3 + threshold * IQR\n",
    "        mask = (df_copy[column] >= lower_bound) & (df_copy[column] <= upper_bound)\n",
    "    elif method == 'zscore':\n",
    "        z_scores = np.abs((df_copy[column] - df_copy[column].mean()) / df_copy[column].std())\n",
    "        mask = z_scores <= threshold\n",
    "    \n",
    "    return df_copy[mask]\n",
    "\n",
    "def add_date_features(df, date_column='date'):\n",
    "    \"\"\"\n",
    "    Add useful date-based features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with date column\n",
    "    date_column : str\n",
    "        Name of the date column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with additional date features\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy['year'] = df_copy[date_column].dt.year\n",
    "    df_copy['month'] = df_copy[date_column].dt.month\n",
    "    df_copy['day'] = df_copy[date_column].dt.day\n",
    "    df_copy['day_of_week'] = df_copy[date_column].dt.dayofweek\n",
    "    df_copy['week_of_year'] = df_copy[date_column].dt.isocalendar().week\n",
    "    df_copy['is_weekend'] = df_copy['day_of_week'] >= 5\n",
    "    df_copy['quarter'] = df_copy[date_column].dt.quarter\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Test preprocessing\n",
    "print(\"Preprocessing Pipeline\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = handle_missing_values(df, strategy='median', columns=['quantity'])\n",
    "print(f\"\\nAfter handling missing values:\")\n",
    "print(f\"Missing quantities: {df_clean['quantity'].isnull().sum()}\")\n",
    "\n",
    "# Remove outliers\n",
    "original_size = len(df_clean)\n",
    "df_clean = remove_outliers(df_clean, 'quantity', method='iqr')\n",
    "print(f\"\\nAfter removing outliers:\")\n",
    "print(f\"Removed {original_size - len(df_clean)} rows ({((original_size - len(df_clean))/original_size)*100:.2f}%)\")\n",
    "\n",
    "# Add date features\n",
    "df_clean = add_date_features(df_clean)\n",
    "print(f\"\\nAfter adding date features:\")\n",
    "print(f\"New columns: {[col for col in df_clean.columns if col not in df.columns]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd584a8",
   "metadata": {},
   "source": [
    "### Module 3: Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eb0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be in: analysis.py\n",
    "\n",
    "def sales_by_period(df, period='month', value_col='revenue'):\n",
    "    \"\"\"\n",
    "    Aggregate sales by time period.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Sales DataFrame with date column\n",
    "    period : str\n",
    "        Time period: 'day', 'week', 'month', 'quarter'\n",
    "    value_col : str\n",
    "        Column to aggregate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Aggregated sales data\n",
    "    \"\"\"\n",
    "    if period == 'day':\n",
    "        grouped = df.groupby('date')\n",
    "    elif period == 'week':\n",
    "        grouped = df.groupby('week_of_year')\n",
    "    elif period == 'month':\n",
    "        grouped = df.groupby('month')\n",
    "    elif period == 'quarter':\n",
    "        grouped = df.groupby('quarter')\n",
    "    \n",
    "    result = grouped[value_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "    return result\n",
    "\n",
    "def product_performance(df):\n",
    "    \"\"\"\n",
    "    Analyze performance by product.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Sales DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Product performance metrics\n",
    "    \"\"\"\n",
    "    performance = df.groupby('product').agg({\n",
    "        'quantity': ['sum', 'mean', 'std'],\n",
    "        'revenue': ['sum', 'mean', 'std'],\n",
    "        'price': 'mean'\n",
    "    }).round(2)\n",
    "    \n",
    "    performance.columns = ['_'.join(col).strip() for col in performance.columns.values]\n",
    "    return performance.sort_values('revenue_sum', ascending=False)\n",
    "\n",
    "def weekend_vs_weekday_analysis(df):\n",
    "    \"\"\"\n",
    "    Compare weekend vs weekday sales.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Sales DataFrame with is_weekend column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Comparison statistics\n",
    "    \"\"\"\n",
    "    comparison = df.groupby('is_weekend').agg({\n",
    "        'quantity': ['sum', 'mean'],\n",
    "        'revenue': ['sum', 'mean']\n",
    "    }).round(2)\n",
    "    \n",
    "    comparison.index = ['Weekday', 'Weekend']\n",
    "    return comparison\n",
    "\n",
    "def calculate_growth_rate(df, period='month', value_col='revenue'):\n",
    "    \"\"\"\n",
    "    Calculate period-over-period growth rate.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Sales DataFrame\n",
    "    period : str\n",
    "        Time period for calculation\n",
    "    value_col : str\n",
    "        Column to calculate growth for\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Growth rates by period\n",
    "    \"\"\"\n",
    "    period_sales = sales_by_period(df, period, value_col)\n",
    "    period_sales['growth_rate'] = period_sales['sum'].pct_change() * 100\n",
    "    return period_sales\n",
    "\n",
    "# Test analysis functions\n",
    "print(\"Sales Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nMonthly Sales Summary:\")\n",
    "monthly = sales_by_period(df_clean, period='month')\n",
    "print(monthly)\n",
    "\n",
    "print(\"\\nProduct Performance:\")\n",
    "products = product_performance(df_clean)\n",
    "print(products)\n",
    "\n",
    "print(\"\\nWeekend vs Weekday:\")\n",
    "weekend_comparison = weekend_vs_weekday_analysis(df_clean)\n",
    "print(weekend_comparison)\n",
    "\n",
    "print(\"\\nMonthly Growth Rate:\")\n",
    "growth = calculate_growth_rate(df_clean, period='month')\n",
    "print(growth[['month', 'sum', 'growth_rate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40ccbfa",
   "metadata": {},
   "source": [
    "### Module 4: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would be in: visualization.py\n",
    "\n",
    "def plot_time_series(df, date_col='date', value_col='revenue', title='Sales Over Time'):\n",
    "    \"\"\"\n",
    "    Plot time series data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Data to plot\n",
    "    date_col : str\n",
    "        Date column name\n",
    "    value_col : str\n",
    "        Value column name\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    daily_sales = df.groupby(date_col)[value_col].sum().reset_index()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(daily_sales[date_col], daily_sales[value_col], linewidth=2)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Date', fontsize=12)\n",
    "    plt.ylabel(value_col.capitalize(), fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_product_comparison(df, metric='revenue'):\n",
    "    \"\"\"\n",
    "    Plot comparison between products.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Sales DataFrame\n",
    "    metric : str\n",
    "        Metric to compare: 'revenue' or 'quantity'\n",
    "    \"\"\"\n",
    "    product_totals = df.groupby('product')[metric].sum().sort_values(ascending=True)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    product_totals.plot(kind='barh', color='steelblue')\n",
    "    plt.title(f'Total {metric.capitalize()} by Product', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(metric.capitalize(), fontsize=12)\n",
    "    plt.ylabel('Product', fontsize=12)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution(df, column, bins=30, title=None):\n",
    "    \"\"\"\n",
    "    Plot distribution of a numerical column.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame\n",
    "    column : str\n",
    "        Column to plot\n",
    "    bins : int\n",
    "        Number of histogram bins\n",
    "    title : str, optional\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.hist(df[column].dropna(), bins=bins, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    \n",
    "    # Add mean and median lines\n",
    "    mean_val = df[column].mean()\n",
    "    median_val = df[column].median()\n",
    "    plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    plt.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "    \n",
    "    plt.title(title or f'Distribution of {column}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(column.capitalize(), fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "print(\"Creating Visualizations...\\n\")\n",
    "\n",
    "# Time series\n",
    "plot_time_series(df_clean, value_col='revenue', title='Daily Revenue Over Time')\n",
    "\n",
    "# Product comparison\n",
    "plot_product_comparison(df_clean, metric='revenue')\n",
    "\n",
    "# Distribution\n",
    "plot_distribution(df_clean, 'quantity', bins=30, title='Distribution of Daily Quantities')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08adc5ee",
   "metadata": {},
   "source": [
    "## Part 3: Complete Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1445970",
   "metadata": {},
   "source": [
    "### Bringing It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5db471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sales_analysis(df, output_report=True):\n",
    "    \"\"\"\n",
    "    Complete end-to-end sales analysis pipeline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Raw sales data\n",
    "    output_report : bool\n",
    "        Whether to print detailed report\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing all analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if output_report:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"COMPLETE SALES ANALYSIS REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Data Quality\n",
    "    if output_report:\n",
    "        print(\"\\n[1] DATA QUALITY ASSESSMENT\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    quality_report = data_quality_report(df)\n",
    "    results['quality'] = quality_report\n",
    "    \n",
    "    if output_report:\n",
    "        print(f\"Total Rows: {quality_report['total_rows']:,}\")\n",
    "        print(f\"Total Columns: {quality_report['total_columns']}\")\n",
    "        print(f\"Duplicates: {quality_report['duplicates']}\")\n",
    "        print(f\"Memory Usage: {quality_report['memory_usage']:.2f} MB\")\n",
    "        \n",
    "        for col, info in quality_report['outliers'].items():\n",
    "            if info['count'] > 0:\n",
    "                print(f\"\\nOutliers in {col}: {info['count']} ({info['percentage']:.2f}%)\")\n",
    "    \n",
    "    # Step 2: Preprocessing\n",
    "    if output_report:\n",
    "        print(\"\\n[2] DATA PREPROCESSING\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    df_processed = handle_missing_values(df, strategy='median')\n",
    "    original_size = len(df_processed)\n",
    "    df_processed = remove_outliers(df_processed, 'quantity')\n",
    "    df_processed = add_date_features(df_processed)\n",
    "    \n",
    "    results['processed_data'] = df_processed\n",
    "    \n",
    "    if output_report:\n",
    "        print(f\"Rows after preprocessing: {len(df_processed):,}\")\n",
    "        print(f\"Rows removed: {original_size - len(df_processed)} ({((original_size - len(df_processed))/original_size)*100:.2f}%)\")\n",
    "    \n",
    "    # Step 3: Descriptive Statistics\n",
    "    if output_report:\n",
    "        print(\"\\n[3] DESCRIPTIVE STATISTICS\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    stats = {\n",
    "        'total_revenue': df_processed['revenue'].sum(),\n",
    "        'avg_daily_revenue': df_processed.groupby('date')['revenue'].sum().mean(),\n",
    "        'total_quantity': df_processed['quantity'].sum(),\n",
    "        'avg_daily_quantity': df_processed.groupby('date')['quantity'].sum().mean(),\n",
    "        'unique_products': df_processed['product'].nunique(),\n",
    "        'date_range_days': (df_processed['date'].max() - df_processed['date'].min()).days\n",
    "    }\n",
    "    results['statistics'] = stats\n",
    "    \n",
    "    if output_report:\n",
    "        print(f\"Total Revenue: ${stats['total_revenue']:,.2f}\")\n",
    "        print(f\"Avg Daily Revenue: ${stats['avg_daily_revenue']:,.2f}\")\n",
    "        print(f\"Total Quantity Sold: {stats['total_quantity']:,.0f}\")\n",
    "        print(f\"Avg Daily Quantity: {stats['avg_daily_quantity']:,.0f}\")\n",
    "        print(f\"Products: {stats['unique_products']}\")\n",
    "        print(f\"Date Range: {stats['date_range_days']} days\")\n",
    "    \n",
    "    # Step 4: Product Analysis\n",
    "    if output_report:\n",
    "        print(\"\\n[4] PRODUCT PERFORMANCE\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    product_perf = product_performance(df_processed)\n",
    "    results['product_performance'] = product_perf\n",
    "    \n",
    "    if output_report:\n",
    "        print(product_perf)\n",
    "    \n",
    "    # Step 5: Time-based Analysis\n",
    "    if output_report:\n",
    "        print(\"\\n[5] TEMPORAL ANALYSIS\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    monthly_sales = sales_by_period(df_processed, period='month')\n",
    "    weekend_analysis = weekend_vs_weekday_analysis(df_processed)\n",
    "    \n",
    "    results['monthly_sales'] = monthly_sales\n",
    "    results['weekend_analysis'] = weekend_analysis\n",
    "    \n",
    "    if output_report:\n",
    "        print(\"\\nWeekend vs Weekday Sales:\")\n",
    "        print(weekend_analysis)\n",
    "        \n",
    "        weekend_revenue = weekend_analysis.loc['Weekend', ('revenue', 'sum')]\n",
    "        weekday_revenue = weekend_analysis.loc['Weekday', ('revenue', 'sum')]\n",
    "        weekend_boost = ((weekend_revenue - weekday_revenue) / weekday_revenue) * 100\n",
    "        print(f\"\\nWeekend Revenue Boost: {weekend_boost:.2f}%\")\n",
    "    \n",
    "    # Step 6: Growth Analysis\n",
    "    if output_report:\n",
    "        print(\"\\n[6] GROWTH ANALYSIS\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    growth = calculate_growth_rate(df_processed, period='month')\n",
    "    results['growth'] = growth\n",
    "    \n",
    "    if output_report:\n",
    "        avg_growth = growth['growth_rate'].mean()\n",
    "        print(f\"Average Monthly Growth Rate: {avg_growth:.2f}%\")\n",
    "        print(\"\\nMonthly Growth Rates:\")\n",
    "        for _, row in growth.iterrows():\n",
    "            if not pd.isna(row['growth_rate']):\n",
    "                print(f\"  Month {int(row['month'])}: {row['growth_rate']:+.2f}%\")\n",
    "    \n",
    "    if output_report:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ANALYSIS COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run complete analysis\n",
    "results = complete_sales_analysis(df, output_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b912f79",
   "metadata": {},
   "source": [
    "## Part 4: How to Organize as a Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866244d",
   "metadata": {},
   "source": [
    "### Recommended Package Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff5e20",
   "metadata": {},
   "source": [
    "If we were to organize all the above code into a proper package:\n",
    "\n",
    "```\n",
    "sales_analytics/\n",
    "├── README.md\n",
    "├── requirements.txt\n",
    "├── setup.py\n",
    "├── sales_analytics/\n",
    "│   ├── __init__.py\n",
    "│   ├── quality/\n",
    "│   │   ├── __init__.py\n",
    "│   │   └── checks.py          # Data quality functions\n",
    "│   ├── preprocessing/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── cleaning.py        # Missing values, outliers\n",
    "│   │   └── features.py        # Feature engineering\n",
    "│   ├── analysis/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── descriptive.py     # Descriptive statistics\n",
    "│   │   ├── temporal.py        # Time-based analysis\n",
    "│   │   └── products.py        # Product analysis\n",
    "│   ├── visualization/\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── plots.py           # Plotting functions\n",
    "│   │   └── dashboards.py      # Dashboard creation\n",
    "│   └── pipeline.py            # Complete pipeline\n",
    "├── tests/\n",
    "│   ├── __init__.py\n",
    "│   ├── test_quality.py\n",
    "│   ├── test_preprocessing.py\n",
    "│   └── test_analysis.py\n",
    "└── examples/\n",
    "    ├── basic_usage.py\n",
    "    └── advanced_analysis.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6ba3c",
   "metadata": {},
   "source": [
    "### Example `__init__.py` Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b0c207",
   "metadata": {},
   "source": [
    "**`sales_analytics/__init__.py`**\n",
    "```python\n",
    "\"\"\"\n",
    "Sales Analytics Package\n",
    "\n",
    "A comprehensive package for sales data analysis including:\n",
    "- Data quality assessment\n",
    "- Preprocessing and cleaning\n",
    "- Statistical analysis\n",
    "- Visualization\n",
    "\"\"\"\n",
    "\n",
    "__version__ = '1.0.0'\n",
    "__author__ = 'Data Analysis Team'\n",
    "\n",
    "# Import key functions to package level\n",
    "from .quality.checks import check_missing_values, detect_outliers_iqr\n",
    "from .preprocessing.cleaning import handle_missing_values, remove_outliers\n",
    "from .preprocessing.features import add_date_features\n",
    "from .analysis.descriptive import sales_by_period, product_performance\n",
    "from .visualization.plots import plot_time_series, plot_product_comparison\n",
    "from .pipeline import complete_sales_analysis\n",
    "\n",
    "__all__ = [\n",
    "    'check_missing_values',\n",
    "    'detect_outliers_iqr',\n",
    "    'handle_missing_values',\n",
    "    'remove_outliers',\n",
    "    'add_date_features',\n",
    "    'sales_by_period',\n",
    "    'product_performance',\n",
    "    'plot_time_series',\n",
    "    'plot_product_comparison',\n",
    "    'complete_sales_analysis'\n",
    "]\n",
    "```\n",
    "\n",
    "**`sales_analytics/quality/__init__.py`**\n",
    "```python\n",
    "\"\"\"Data quality assessment module.\"\"\"\n",
    "\n",
    "from .checks import (\n",
    "    check_missing_values,\n",
    "    detect_outliers_iqr,\n",
    "    data_quality_report\n",
    ")\n",
    "\n",
    "__all__ = [\n",
    "    'check_missing_values',\n",
    "    'detect_outliers_iqr',\n",
    "    'data_quality_report'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3ce1f",
   "metadata": {},
   "source": [
    "### Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600454c2",
   "metadata": {},
   "source": [
    "Once organized as a package, users could use it like this:\n",
    "\n",
    "```python\n",
    "# Simple usage - import main functions\n",
    "import sales_analytics as sa\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('sales_data.csv')\n",
    "\n",
    "# Quick analysis\n",
    "results = sa.complete_sales_analysis(df)\n",
    "\n",
    "# Or use specific functions\n",
    "df_clean = sa.handle_missing_values(df)\n",
    "df_clean = sa.remove_outliers(df_clean, 'quantity')\n",
    "sa.plot_time_series(df_clean)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Advanced usage - import specific modules\n",
    "from sales_analytics import preprocessing, analysis, visualization\n",
    "\n",
    "# Preprocessing\n",
    "df_clean = preprocessing.handle_missing_values(df)\n",
    "df_clean = preprocessing.add_date_features(df_clean)\n",
    "\n",
    "# Analysis\n",
    "monthly = analysis.sales_by_period(df_clean, 'month')\n",
    "products = analysis.product_performance(df_clean)\n",
    "\n",
    "# Visualization\n",
    "visualization.plot_time_series(df_clean)\n",
    "visualization.plot_product_comparison(df_clean)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe289df8",
   "metadata": {},
   "source": [
    "## Key Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbfcb8",
   "metadata": {},
   "source": [
    "✅ **Integration with Data Science Libraries:**\n",
    "- Use standard aliases: `np`, `pd`, `plt`\n",
    "- Leverage NumPy and Pandas for efficiency\n",
    "- Build custom functions on top of these libraries\n",
    "\n",
    "✅ **Modular Code Organization:**\n",
    "- Separate concerns (quality, preprocessing, analysis, visualization)\n",
    "- Each module has clear, single purpose\n",
    "- Functions are reusable and testable\n",
    "\n",
    "✅ **Complete Analysis Pipeline:**\n",
    "- Data quality assessment\n",
    "- Preprocessing and cleaning\n",
    "- Statistical analysis\n",
    "- Visualization\n",
    "- Reproducible results\n",
    "\n",
    "✅ **Package Structure:**\n",
    "- Logical directory hierarchy\n",
    "- Clear naming conventions\n",
    "- Proper use of `__init__.py`\n",
    "- Expose high-level API\n",
    "\n",
    "✅ **Best Practices:**\n",
    "- Comprehensive docstrings\n",
    "- Type hints where appropriate\n",
    "- Error handling\n",
    "- Consistent coding style\n",
    "- Documentation and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c302aa0",
   "metadata": {},
   "source": [
    "## Final Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41255126",
   "metadata": {},
   "source": [
    "### Exercise 1: Extend the Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e51787",
   "metadata": {},
   "source": [
    "Add a new analysis module with functions to:\n",
    "1. Calculate customer lifetime value (if customer data available)\n",
    "2. Perform cohort analysis\n",
    "3. Identify best-selling product combinations\n",
    "4. Forecast future sales using moving averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ad78cd",
   "metadata": {},
   "source": [
    "### Exercise 2: Create Export Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b08c3d5",
   "metadata": {},
   "source": [
    "Add a new `export` module with functions to:\n",
    "1. Export analysis results to CSV\n",
    "2. Generate PDF reports with plots\n",
    "3. Create Excel workbooks with multiple sheets\n",
    "4. Export visualizations as PNG/SVG files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181b4223",
   "metadata": {},
   "source": [
    "### Exercise 3: Build Your Own Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813eeca",
   "metadata": {},
   "source": [
    "Choose a domain (e.g., text analysis, image processing, financial analysis) and:\n",
    "1. Design a complete package structure\n",
    "2. Implement at least 3 submodules with 2-3 functions each\n",
    "3. Create proper `__init__.py` files\n",
    "4. Write comprehensive docstrings\n",
    "5. Include a usage example\n",
    "6. Test your package by importing and using it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd840439",
   "metadata": {},
   "source": [
    "### Exercise 4: Package Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d3480c",
   "metadata": {},
   "source": [
    "For the sales analytics package, create:\n",
    "1. A comprehensive `README.md` with:\n",
    "   - Installation instructions\n",
    "   - Quick start guide\n",
    "   - Usage examples\n",
    "   - API reference\n",
    "2. A `requirements.txt` file listing all dependencies\n",
    "3. Example notebooks demonstrating key features\n",
    "4. A `CHANGELOG.md` for version history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afc18a",
   "metadata": {},
   "source": [
    "## Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f9686d",
   "metadata": {},
   "source": [
    "You've completed the comprehensive modules and packages series! You now understand:\n",
    "\n",
    "✅ What modules and packages are and why they're important\n",
    "\n",
    "✅ Different importing strategies and when to use each\n",
    "\n",
    "✅ Python's standard library modules for data analysis\n",
    "\n",
    "✅ How to create your own modules with proper structure\n",
    "\n",
    "✅ How to organize code into packages with hierarchies\n",
    "\n",
    "✅ How to integrate with NumPy, Pandas, and Matplotlib\n",
    "\n",
    "✅ Best practices for documentation and code organization\n",
    "\n",
    "✅ How to build complete, professional data analysis workflows\n",
    "\n",
    "**Next Steps:**\n",
    "- Practice by building your own packages\n",
    "- Explore popular packages on PyPI to see how they're structured\n",
    "- Learn about package distribution with `setuptools` and `pip`\n",
    "- Study open-source projects to see real-world package design\n",
    "- Consider contributing to existing packages to deepen your understanding"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
